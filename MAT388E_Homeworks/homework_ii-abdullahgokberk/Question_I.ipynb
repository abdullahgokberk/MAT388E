{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "## \"Regularized Logistic Regression\"\n",
    "\n",
    "**author:** \"Gökberk Abdullah\" \n",
    "\n",
    "**school number:** \"090170341\"\n",
    "\n",
    "**date:** \"May 8, 2023\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Find a data set which is suitable for binary classification (there is no limit on the number of features or type of the features). Then, apply logistic regression with l1 norm, l2 norm, and elastic-net to this data set to find the best performing model with respect to a classification metric. Explain the reason (give support) why you picked that model as the final one.\n",
    "\n",
    "#### Instructions to follow:\n",
    "\n",
    "- Include all your codes here. Be sure that your code is CLEAN, READABLE, and REPRODUCIBLE.\n",
    "- Put your data set into a **datasets** folder.\n",
    "- Put your images (if available) into an **images** folder.\n",
    "- Please return a NICE and CLEAR homework. Otherwise, it will not be graded.\n",
    "- Please write YOUR OWN code. **DO NOT copy** my codes or someone else's codes.\n",
    "- Please **DO NOT use** IRIS data set for this task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Decription\n",
    "\n",
    "In the above reference, the dataset was created using Portuguese “Vinho Verde” red wine varieties. Inputs include objective tests (eg PH values) and output is based on sensory data. (median of at least 3 reviews by wine experts). Each expert rated the wine quality from 0 (very poor) to 10 (very excellent). Due to privacy and logistical issues, only physicochemical (inputs) and sensory (output) variables are available (e.g. no data on grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "### List and Description of Data\n",
    "\n",
    "Inputs (based on physicochemical tests):\n",
    "\n",
    "* fixed acidity: most wine-related acids are stable and non-volatile (does not evaporate easily).\n",
    "\n",
    "* volatile acidity: the amount of acetic acid in wine, which at very high levels can cause an unpleasant vinegar flavor.\n",
    "\n",
    "* citric acid: small amounts of citric acid can add 'freshness' and flavor to wines.\n",
    "\n",
    "* residual sugar: amount of sugar remaining after fermentation has stopped, less than 1 gram/liter of wine is rare to find and wines over 45 grams/liter are considered sweet.\n",
    "\n",
    "* chlorides: the amount of salt in the wine.\n",
    "\n",
    "* free sulfur dioxide: The free form of SO2 is in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; prevents microbial growth and oxidation of wine.\n",
    "\n",
    "* total sulfur dioxide: the amount of free and bound forms of SO2 at low concentrations; SO2 is mostly undetectable in wine, but at concentrations of free SO2 above 50 ppm, SO2 becomes evident in the nose and flavor of the wine.\n",
    "\n",
    "* density: The density of water is close to water, based on percent alcohol and sugar content.\n",
    "\n",
    "* pH: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3–4 on the pH scale.\n",
    "\n",
    "* sulphates: A wine additive that can contribute to levels of sulfur dioxide gas (SO2), acting as an antimicrobial and antioxidant\n",
    "\n",
    "* alcohol: percent alcohol content of wine\n",
    "\n",
    "Output (based on sensory data):\n",
    "\n",
    "* quality (score from 0 to 10)\n",
    "\n",
    "You can find the source of the data on [this](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009) site. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"datasets\", \"winequality-red.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define wines rated 7 and above as good wine and wines rated below 7 as bad wine.\n",
    "In this way, we now have the dataset suitable for binary classification as requested in the question.\n",
    "\n",
    "With `plot.barh()` I can observe the number of elements in our target variables categories.\n",
    "\n",
    "Seems to be mostly made up of poor quality wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1382\n",
       "1     217\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"quality\"] = df[\"quality\"].apply(lambda x: 1 if x>6 else 0)\n",
    "df[\"quality\"].value_counts().plot.barh();\n",
    "df[\"quality\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get general information with the `info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the independent variables are of float type and we can get more detailed information with the `describe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>8.319637</td>\n",
       "      <td>1.741096</td>\n",
       "      <td>4.60000</td>\n",
       "      <td>7.1000</td>\n",
       "      <td>7.90000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>15.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.52000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1.58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>2.20000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>15.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.07900</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.61100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>72.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>289.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.99007</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>0.99675</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>1.00369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>2.74000</td>\n",
       "      <td>3.2100</td>\n",
       "      <td>3.31000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>4.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>8.40000</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>10.20000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>14.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>1599.0</td>\n",
       "      <td>0.135710</td>\n",
       "      <td>0.342587</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count       mean        std      min      25%  \\\n",
       "fixed acidity         1599.0   8.319637   1.741096  4.60000   7.1000   \n",
       "volatile acidity      1599.0   0.527821   0.179060  0.12000   0.3900   \n",
       "citric acid           1599.0   0.270976   0.194801  0.00000   0.0900   \n",
       "residual sugar        1599.0   2.538806   1.409928  0.90000   1.9000   \n",
       "chlorides             1599.0   0.087467   0.047065  0.01200   0.0700   \n",
       "free sulfur dioxide   1599.0  15.874922  10.460157  1.00000   7.0000   \n",
       "total sulfur dioxide  1599.0  46.467792  32.895324  6.00000  22.0000   \n",
       "density               1599.0   0.996747   0.001887  0.99007   0.9956   \n",
       "pH                    1599.0   3.311113   0.154386  2.74000   3.2100   \n",
       "sulphates             1599.0   0.658149   0.169507  0.33000   0.5500   \n",
       "alcohol               1599.0  10.422983   1.065668  8.40000   9.5000   \n",
       "quality               1599.0   0.135710   0.342587  0.00000   0.0000   \n",
       "\n",
       "                           50%        75%        max  \n",
       "fixed acidity          7.90000   9.200000   15.90000  \n",
       "volatile acidity       0.52000   0.640000    1.58000  \n",
       "citric acid            0.26000   0.420000    1.00000  \n",
       "residual sugar         2.20000   2.600000   15.50000  \n",
       "chlorides              0.07900   0.090000    0.61100  \n",
       "free sulfur dioxide   14.00000  21.000000   72.00000  \n",
       "total sulfur dioxide  38.00000  62.000000  289.00000  \n",
       "density                0.99675   0.997835    1.00369  \n",
       "pH                     3.31000   3.400000    4.01000  \n",
       "sulphates              0.62000   0.730000    2.00000  \n",
       "alcohol               10.20000  11.100000   14.90000  \n",
       "quality                0.00000   0.000000    1.00000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on Elastic-Net Regularization\n",
    "\n",
    "Logistic regression is a common machine learning algorithm used for classification tasks. One potential issue with logistic regression is overfitting, where the model becomes too complex and begins to fit the noise in the data. Regularization is a technique used to address this issue by adding a penalty term to the objective function of the model.\n",
    "\n",
    "Elastic Net regularization is a type of regularization that combines L1 (Lasso) and L2 (Ridge) regularization. L1 regularization helps with feature selection by driving some of the coefficients to zero, while L2 regularization helps with reducing the magnitude of the coefficients. Elastic Net regularization combines the advantages of both L1 and L2 regularization methods and allows for a balance between feature selection and coefficient magnitude reduction.\n",
    "\n",
    "The Elastic Net regularization method is particularly useful when there are many features in the dataset and some of them are more important than others. The L1 regularization of the Elastic Net helps to identify and select the most important features, while the L2 regularization reduces the magnitudes of the coefficients, thus reducing the risk of overfitting.\n",
    "\n",
    "One advantage of Elastic Net regularization is that it provides a good balance between feature selection and coefficient magnitude reduction. However, it does require hyperparameter tuning, which can be challenging. Selecting the right hyperparameters is important for the model's performance. Additionally, Elastic Net regularization can result in a model that is difficult to interpret, as some coefficients may be set to zero, making it challenging to understand the impact of the corresponding features on the outcome variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide our data into train and test.\n",
    "\n",
    "Thanks to the `stratify` parameter, `X_test` and `X_train` will contain 0 and 1 in the same ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a pipeline combining standardization and logistic regression using the `make_pipeline()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scareg_pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal parameter values ​​using the `GridSearchCV()` function. This function will try a number of different combinations of hyperparameter values ​​to determine the parameters that give the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "60 fits failed out of a total of 135.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.87099571 0.87099571        nan 0.87334559 0.87412684 0.87412684\n",
      "        nan        nan        nan 0.8772549  0.87647365        nan\n",
      " 0.87334865 0.87334865 0.87334865        nan        nan        nan\n",
      " 0.8741299  0.8741299         nan 0.8741299  0.8741299  0.8756924\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logisticregression&#x27;,\n",
       "                                        LogisticRegression())]),\n",
       "             param_grid={&#x27;logisticregression__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logisticregression__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;,\n",
       "                                                         &#x27;elasticnet&#x27;],\n",
       "                         &#x27;logisticregression__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;,\n",
       "                                                        &#x27;lbfgs&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;logisticregression&#x27;,\n",
       "                                        LogisticRegression())]),\n",
       "             param_grid={&#x27;logisticregression__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;logisticregression__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;,\n",
       "                                                         &#x27;elasticnet&#x27;],\n",
       "                         &#x27;logisticregression__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;,\n",
       "                                                        &#x27;lbfgs&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression())]),\n",
       "             param_grid={'logisticregression__C': [0.1, 1, 10],\n",
       "                         'logisticregression__penalty': ['l1', 'l2',\n",
       "                                                         'elasticnet'],\n",
       "                         'logisticregression__solver': ['liblinear', 'saga',\n",
       "                                                        'lbfgs']})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'logisticregression__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'logisticregression__solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "              'logisticregression__C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(scareg_pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the best parameter values ​​using `best_params_` and the best score using `best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'logisticregression__C': 1, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'}\n",
      "Best score: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the best parameters:\n",
    "\n",
    "The `grid.best_score_` value is the best average accuracy value obtained during the cross-validation process. This value is obtained as a result of dividing the training data into different parts and training and validating the model sequentially on each part, and reflects the performance of the model in general.\n",
    "\n",
    "The `accuracy_score` shows the correct classification rate of the model over all of the training data. Therefore, `grid.best_score_` and accuracy_score may give different values. `grid.best_score_` gives the average of the correct classification rates, while `accuracy_score` gives the correct classification rate only once.\n",
    "\n",
    "Best score: 0.88 but accuracy score: 0.89 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test set: 0.89375\n",
      "Accuracy for train set: 0.8819390148553558\n"
     ]
    }
   ],
   "source": [
    "scareg_pipe_final = make_pipeline(StandardScaler(), LogisticRegression( C=1, penalty ='l2', solver ='saga'))\n",
    "scareg_pipe_final.fit(X_train,y_train)\n",
    "y_pred = scareg_pipe_final.predict(X_test)\n",
    "print(\"Accuracy for test set:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_pred_train = scareg_pipe_final.predict(X_train)\n",
    "print(\"Accuracy for train set:\", accuracy_score(y_train, y_pred_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[270   7]\n",
      " [ 27  16]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       277\n",
      "           1       0.70      0.37      0.48        43\n",
      "\n",
      "    accuracy                           0.89       320\n",
      "   macro avg       0.80      0.67      0.71       320\n",
      "weighted avg       0.88      0.89      0.88       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first calculates the positive class probabilities predicted by the model on the test set, then calculates the `ROC` curve and `AUC` score. After calculating the false positive rate (fpr) and true positive rate (tpr) values ​​for the ROC curve, we calculate the `AUC` score using the `auc()` function. 0.879 is very good score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.879523129879943\n"
     ]
    }
   ],
   "source": [
    "y_prob = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"AUC score:\", roc_auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I selected this model as the final model because it had the highest accuracy score of all the models that were tested. Specifically, this model was fit with a penalty of 'l1', C value of 1, and solver 'liblinear'.\n",
    "\n",
    "I chose to use an L1 penalty instead of an L2 penalty because L1 regularization can help with feature selection by setting the coefficients of irrelevant features to zero, which can improve the model's performance and make it more interpretable. In addition, I chose a C value of 1, which is the default value in logistic regression and indicates that there is no regularization strength adjustment.\n",
    "\n",
    "Finally, I selected the solver 'liblinear' because it is efficient for small datasets and is the default solver for L1 regularization. It is also a good choice when the number of features is large relative to the number of samples. Overall, I believe this combination of hyperparameters produced the best model for this particular dataset and classification problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report:\n",
    "\n",
    "Additionaly, the classification report shows the precision, recall, and F1-score for each class. In this case, we have two classes: 0 and 1, where 0 represents low-quality wines and 1 represents high-quality wines.\n",
    "\n",
    "For the low-quality wines (class 0), the model achieved a precision of 0.91, which means that 91% of the wines predicted as low-quality were actually low-quality. The recall was 0.97, which means that 97% of the actual low-quality wines were correctly identified by the model. The F1-score was 0.94, which is the harmonic mean of precision and recall, and provides a single score that balances both metrics.\n",
    "\n",
    "For the high-quality wines (class 1), the model achieved a precision of 0.70, which means that 70% of the wines predicted as high-quality were actually high-quality. The recall was 0.37, which means that only 37% of the actual high-quality wines were correctly identified by the model. The F1-score was 0.48, which is relatively low compared to the low-quality wines.\n",
    "\n",
    "Overall, the model performed better at identifying low-quality wines than high-quality wines. This could be due to the class imbalance in the dataset, where there were many more low-quality wines than high-quality wines. The precision and recall scores for the low-quality wines were both above 0.9, which indicates that the model was highly accurate at identifying them. However, the precision and recall scores for the high-quality wines were both below 0.8, which indicates that the model had more difficulty identifying them correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
